{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, recall_score, make_scorer, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import statsmodels.api as sm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/students_mental_health_survey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a total of 27 rows with null values so decided to drop all 27 rows. \n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sleep_Quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Physical_Activity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Diet_Quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Social_Support'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Relationship_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Substance_Use'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Counseling_Service_Use'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Family_History'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Chronic_Illness'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Financial_Stress'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Extracurricular_Involvement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Semester_Credit_Load'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Extracurricular_Involvement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Depression_Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which columns are categorical and continuous\n",
    "ohe_cols = ['Course', 'Gender', 'Relationship_Status', 'Family_History', 'Residence_Type', 'Chronic_Illness']\n",
    "ordinal_cols = ['Stress_Level', 'Anxiety_Score','Sleep_Quality', \n",
    "               'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Substance_Use', \n",
    "               'Counseling_Service_Use', 'Extracurricular_Involvement']\n",
    "stress_ord = [0, 1, 2, 3, 4, 5]\n",
    "anx_ord = [0, 1, 2, 3, 4, 5]\n",
    "sleep_ord = ['Poor', 'Average', 'Good']\n",
    "phys_ord = ['Low', 'Moderate', 'High']\n",
    "diet_ord = ['Poor', 'Average', 'Good']\n",
    "social_ord = ['Low', 'Moderate', 'High']\n",
    "subst_ord = ['Never', 'Occasionally', 'Frequently']\n",
    "counsel_ord = ['Never', 'Occasionally', 'Frequently']\n",
    "extracurric_ord = ['Low', 'Moderate', 'High']\n",
    "\n",
    "continuous_cols = ['Age', 'Semester_Credit_Load', 'CGPA']\n",
    "\n",
    "# Create a column transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_cols),  # Scale continuous variables\n",
    "        ('ohe', OneHotEncoder(), ohe_cols),\n",
    "        ('ord', OrdinalEncoder(categories=[stress_ord, anx_ord, sleep_ord, phys_ord, diet_ord, \n",
    "                                           social_ord, subst_ord, counsel_ord, extracurric_ord]), \n",
    "         ordinal_cols)])\n",
    "\n",
    "# Create a pipeline that applies the preprocessing steps\n",
    "preprocess_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Transform the data\n",
    "transformed_data = preprocess_pipeline.fit_transform(df)\n",
    "type(transformed_data)\n",
    "\n",
    "# Get the column names for the one-hot encoded categorical variables from the one-hot encoder\n",
    "ohe_encoder = preprocess_pipeline.named_steps['preprocessor'].named_transformers_['ohe']\n",
    "one_hot_feature_names = ohe_encoder.get_feature_names(input_features=ohe_cols)\n",
    "\n",
    "# # Combine the one-hot encoded features with the scaled continuous variables and ordinal encoded features\n",
    "all_feature_names = list(continuous_cols) + list(one_hot_feature_names) + list(ordinal_cols)\n",
    "\n",
    "# # Create a DataFrame using the transformed data and feature names\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=all_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Depression_Score'] >= 3, 'Depression_Binary'] = 'Yes'\n",
    "df.loc[df['Depression_Score'] < 3, 'Depression_Binary'] = 'No'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train_Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformed_df\n",
    "y = df['Depression_Binary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Model Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dummy model that will always predict Yes\n",
    "dummy_model = DummyClassifier(strategy=\"constant\", constant='Yes')\n",
    "dummy_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('pickled_models/dummy_model.pkl', 'wb') as f:\n",
    "    pickle.dump(dummy_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/dummy_model.pkl', 'rb') as f:\n",
    "    dummy_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the predictions from the Pipeline using the training data\n",
    "y_pred = dummy_model.predict(X_train)\n",
    "# Evaluating the accuracy score on the training data\n",
    "print('Recall: ', recall_score(y_train, y_pred, pos_label='Yes'))\n",
    "print('F1: ', f1_score(y_train, y_pred, pos_label='Yes'))\n",
    "print('Accuracy: ', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating confusion matrix\n",
    "cf = confusion_matrix(y_train, y_pred)\n",
    "# Displaying confusion matrix\n",
    "ConfusionMatrixDisplay(cf, display_labels=['No', 'Yes']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scorer = make_scorer(recall_score, pos_label='Yes')\n",
    "f1_scorer = make_scorer(f1_score, pos_label='Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Scores\n",
    "print('CV Recall: ', cross_val_score(dummy_model, X_train, y_train, scoring=recall_scorer).mean())\n",
    "print('CV F1: ', cross_val_score(dummy_model, X_train, y_train, scoring=f1_scorer).mean())\n",
    "print('CV Accuracy: ', cross_val_score(dummy_model, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model: Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating steps for a Pipeline \n",
    "# tree_steps = [('smote', SMOTE(random_state=333)),\n",
    "#               ('tree', DecisionTreeClassifier())]\n",
    "# # Feeding the Pipeline the steps defined above\n",
    "# tree_pipe = Pipeline(tree_steps)\n",
    "# # Fitting the training data to the Pipeline\n",
    "# tree_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pickle the model\n",
    "# with open('tree_pipe.pkl', 'wb') as f:\n",
    "#     pickle.dump(tree_pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/tree_pipe.pkl', 'rb') as f:\n",
    "    tree_pipe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/tree_pipe.pkl', 'rb') as f:\n",
    "    tree_pipe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the predictions from the Pipeline using the training data\n",
    "y_pred = tree_pipe.predict(X_train)\n",
    "# Evaluating the accuracy score on the training data\n",
    "print('Recall: ', recall_score(y_train, y_pred, pos_label='Yes'))\n",
    "print('F1: ', f1_score(y_train, y_pred, pos_label='Yes'))\n",
    "print('Accuracy: ', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating confusion matrix\n",
    "cf = confusion_matrix(y_train, y_pred)\n",
    "# Displaying confusion matrix\n",
    "ConfusionMatrixDisplay(cf, display_labels=['No', 'Yes']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Scores\n",
    "print('CV Recall: ', cross_val_score(tree_pipe, X_train, y_train, scoring=recall_scorer).mean())\n",
    "print('CV F1: ', cross_val_score(tree_pipe, X_train, y_train, scoring=f1_scorer).mean())\n",
    "print('CV Accuracy: ', cross_val_score(tree_pipe, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating steps for a Pipeline \n",
    "forest_steps = [('smote', SMOTE(random_state=333)),\n",
    "                ('forest', RandomForestClassifier(random_state=333))]\n",
    "# Feeding the Pipeline the steps defined above\n",
    "forest_pipe = Pipeline(forest_steps)\n",
    "# Fitting the training data to the Pipeline\n",
    "forest_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('pickled_models/forest_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(forest_pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/forest_pipe.pkl', 'rb') as f:\n",
    "    forest_pipe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for model\n",
    "# Defining the predictions from the Pipeline using the training data\n",
    "y_pred = forest_pipe.predict(X_train)\n",
    "# Evaluating the accuracy score on the training data\n",
    "print('Recall: ', recall_score(y_train, y_pred, pos_label='Yes'))\n",
    "print('F1: ', f1_score(y_train, y_pred, pos_label='Yes'))\n",
    "print('Accuracy: ', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating confusion matrix\n",
    "cf = confusion_matrix(y_train, y_pred)\n",
    "# Displaying confusion matrix\n",
    "ConfusionMatrixDisplay(cf, display_labels=['No', 'Yes']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Scores for model\n",
    "model = forest_pipe\n",
    "print('CV Recall: ', cross_val_score(model, X_train, y_train, scoring=recall_scorer).mean())\n",
    "print('CV F1: ', cross_val_score(model, X_train, y_train, scoring=f1_scorer).mean())\n",
    "print('CV Accuracy: ', cross_val_score(model, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with GridSearch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Recall\n",
    "# Creating parameters for GridSearch\n",
    "params = {'forest__n_estimators': [50, 100, 150],\n",
    "          'forest__criterion': ['gini', 'entropy'],\n",
    "          'forest__max_depth': [10, None],\n",
    "          'forest__min_samples_split': [1, 2],\n",
    "          'forest__min_weight_fraction_leaf': [0, .5],\n",
    "         'forest__max_features': ['auto', None, 15],\n",
    "          'forest__max_leaf_nodes': [None, 10],\n",
    "          'forest__min_impurity_decrease': [0, .5],\n",
    "         }\n",
    "# GridSearch with the random forest pipeline, parameters above, 5 fold cross validation, and accuracy score\n",
    "forest_grid1 = GridSearchCV(estimator=forest_pipe, param_grid=params, cv=5, scoring=recall_scorer, verbose=2)\n",
    "# Fitting the GridSearch\n",
    "forest_grid1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('pickled_models/forest_grid1.pkl', 'wb') as f:\n",
    "    pickle.dump(forest_grid1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/forest_grid1.pkl', 'rb') as f:\n",
    "    forest_grid1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall\")\n",
    "print(forest_grid1.best_estimator_)\n",
    "print(forest_grid1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = forest_grid1.best_estimator_\n",
    "# Cross Validation Scores for model\n",
    "print('CV Recall: ', cross_val_score(model, X_train, y_train, scoring=recall_scorer).mean())\n",
    "print('CV F1: ', cross_val_score(model, X_train, y_train, scoring=f1_scorer).mean())\n",
    "print('CV Accuracy: ', cross_val_score(model, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest with GridSearch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Recall\n",
    "# Creating parameters for GridSearch\n",
    "params = {'forest__n_estimators': [150, 160, 140],\n",
    "          'forest__criterion': ['gini'],\n",
    "          'forest__max_depth': [30, 10],\n",
    "          'forest__min_samples_split': [3, 2],\n",
    "          'forest__min_weight_fraction_leaf': [.75, .5, .25],\n",
    "         'forest__max_features': [30, 20, None],\n",
    "          'forest__max_leaf_nodes': [None, 2],\n",
    "          'forest__min_impurity_decrease': [0, .25],\n",
    "         }\n",
    "# GridSearch with the random forest pipeline, parameters above, 5 fold cross validation, and accuracy score\n",
    "forest_grid2 = GridSearchCV(estimator=forest_pipe, param_grid=params, cv=5, scoring=recall_scorer, verbose=2)\n",
    "# Fitting the GridSearch\n",
    "forest_grid2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('pickled_models/forest_grid2.pkl', 'wb') as f:\n",
    "    pickle.dump(forest_grid2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/forest_grid2.pkl', 'rb') as f:\n",
    "    forest_grid2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall\")\n",
    "print(forest_grid2.best_estimator_)\n",
    "print(forest_grid2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest with Grid Search 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Recall\n",
    "# Creating parameters for GridSearch\n",
    "params = {'forest__n_estimators': [145, 155, 150],\n",
    "          'forest__criterion': ['gini'],\n",
    "          'forest__max_depth': [30, 20, 40],\n",
    "          'forest__min_samples_split': [3, 4],\n",
    "          'forest__min_weight_fraction_leaf': [.2, .3, .25],\n",
    "         'forest__max_features': [30, 40, 50],\n",
    "          'forest__max_leaf_nodes': [1, 2, 3],\n",
    "          'forest__min_impurity_decrease': [0, .1],\n",
    "         }\n",
    "# GridSearch with the random forest pipeline, parameters above, 5 fold cross validation, and accuracy score\n",
    "forest_grid3 = GridSearchCV(estimator=forest_pipe, param_grid=params, cv=5, scoring=recall_scorer, verbose=2)\n",
    "# Fitting the GridSearch\n",
    "forest_grid3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('pickled_models/forest_grid3.pkl', 'wb') as f:\n",
    "    pickle.dump(forest_grid3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/forest_grid3.pkl', 'rb') as f:\n",
    "    forest_grid3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall\")\n",
    "print(forest_grid3.best_estimator_)\n",
    "print(forest_grid3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating steps for a Pipeline \n",
    "logreg_steps = [('smote', SMOTE(random_state=333)),\n",
    "                ('logreg', LogisticRegression(random_state=333))]\n",
    "# Feeding the Pipeline the steps defined above\n",
    "logreg_pipe = Pipeline(logreg_steps)\n",
    "# Fitting the training data to the Pipeline\n",
    "logreg_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('pickled_models/logreg_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg_pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/logreg_pipe.pkl', 'rb') as f:\n",
    "    logreg_pipe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for model\n",
    "# Defining the predictions from the Pipeline using the training data\n",
    "y_pred = logreg_pipe.predict(X_train)\n",
    "# Evaluating the accuracy score on the training data\n",
    "print('Recall: ', recall_score(y_train, y_pred, pos_label='Yes'))\n",
    "print('F1: ', f1_score(y_train, y_pred, pos_label='Yes'))\n",
    "print('Accuracy: ', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating confusion matrix\n",
    "cf = confusion_matrix(y_train, y_pred)\n",
    "# Displaying confusion matrix\n",
    "ConfusionMatrixDisplay(cf, display_labels=['No', 'Yes']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Scores for model\n",
    "model = logreg_pipe\n",
    "print('CV Recall: ', cross_val_score(model, X_train, y_train, scoring=recall_scorer).mean())\n",
    "print('CV F1: ', cross_val_score(model, X_train, y_train, scoring=f1_scorer).mean())\n",
    "print('CV Accuracy: ', cross_val_score(model, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with GridSearch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating parameters for GridSearch\n",
    "logreg_params = {'logreg__class_weight':['balanced', None, [1, 10]],\n",
    "                'logreg__C': [1, 0.0001],\n",
    "                'logreg__solver': ['lbfgs', 'sag', 'saga'],\n",
    "                'logreg__max_iter': [100],\n",
    "                'logreg__penalty': ['l1', 'l2']}\n",
    "# GridSearch with the logistic regression pipeline, parameters above, 5 fold cross validation, and accuracy score\n",
    "logreg_grid1 = GridSearchCV(estimator=logreg_pipe, param_grid=logreg_params, cv=5, scoring=recall_scorer, verbose=2)\n",
    "# Fitting the GridSearch\n",
    "logreg_grid1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('pickled_models/logreg_grid1.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg_grid1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/logreg_grid1.pkl', 'rb') as f:\n",
    "    logreg_grid1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall\")\n",
    "print(logreg_grid1.best_estimator_)\n",
    "print(logreg_grid1.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with GridSearch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating parameters for GridSearch\n",
    "logreg_params = {'logreg__class_weight':['balanced', [1, 20], [20, 1], [1, 50]],\n",
    "                'logreg__C': [1, 10, 100],\n",
    "                'logreg__solver': ['lbfgs', 'liblinear', 'newton-cg'],\n",
    "                'logreg__max_iter': [50, 100, 150, 1000],\n",
    "                'logreg__penalty': ['elasticnet', 'l2', None]}\n",
    "# GridSearch with the logistic regression pipeline, parameters above, 5 fold cross validation, and accuracy score\n",
    "logreg_grid2 = GridSearchCV(estimator=logreg_pipe, param_grid=logreg_params, cv=5, scoring=recall_scorer, verbose=2)\n",
    "# Fitting the GridSearch\n",
    "logreg_grid2.fit(X_train, y_train)\n",
    "\n",
    "#LogReg to see parameters when changing params\n",
    "# LogisticRegression(random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('pickled_models/logreg_grid2.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg_grid2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/logreg_grid2.pkl', 'rb') as f:\n",
    "    logreg_grid2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall\")\n",
    "print(logreg_grid2.best_estimator_)\n",
    "print(logreg_grid2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with GridSearch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating parameters for GridSearch\n",
    "logreg_params = {'logreg__class_weight':['balanced'],\n",
    "                'logreg__C': [1, .01, .001],\n",
    "                'logreg__solver': ['lbfgs'],\n",
    "                'logreg__max_iter': [100, 125, 200, 300],\n",
    "                'logreg__penalty': ['l2']}\n",
    "# GridSearch with the logistic regression pipeline, parameters above, 5 fold cross validation, and accuracy score\n",
    "logreg_grid3 = GridSearchCV(estimator=logreg_pipe, param_grid=logreg_params, cv=5, scoring=recall_scorer, verbose=2)\n",
    "# Fitting the GridSearch\n",
    "logreg_grid3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('pickled_models/logreg_grid3.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg_grid3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/logreg_grid3.pkl', 'rb') as f:\n",
    "    logreg_grid3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall\")\n",
    "print(logreg_grid3.best_estimator_)\n",
    "print(logreg_grid3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Scores for model\n",
    "model = logreg_grid3.best_estimator_\n",
    "print('CV Recall: ', cross_val_score(model, X_train, y_train, scoring=recall_scorer).mean())\n",
    "print('CV F1: ', cross_val_score(model, X_train, y_train, scoring=f1_scorer).mean())\n",
    "print('CV Accuracy: ', cross_val_score(model, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth Model: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating steps for a Pipeline \n",
    "neural_steps = [('smote', SMOTE(random_state=333)),\n",
    "                ('neural', MLPClassifier(random_state=333))]\n",
    "# Feeding the Pipeline the steps defined above\n",
    "neural_pipe = Pipeline(neural_steps)\n",
    "# Fitting the training data to the Pipeline\n",
    "neural_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open('pickled_models/neural_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(neural_pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled model\n",
    "with open('pickled_models/neural_pipe.pkl', 'rb') as f:\n",
    "    neural_pipe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for model\n",
    "# Defining the predictions from the Pipeline using the training data\n",
    "y_pred = neural_pipe.predict(X_train)\n",
    "# Evaluating the accuracy score on the training data\n",
    "print('Recall: ', recall_score(y_train, y_pred, pos_label='Yes'))\n",
    "print('F1: ', f1_score(y_train, y_pred, pos_label='Yes'))\n",
    "print('Accuracy: ', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating confusion matrix\n",
    "cf = confusion_matrix(y_train, y_pred)\n",
    "# Displaying confusion matrix\n",
    "ConfusionMatrixDisplay(cf, display_labels=['No', 'Yes']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Scores for model\n",
    "model = neural_pipe\n",
    "print('CV Recall: ', cross_val_score(model, X_train, y_train, scoring=recall_scorer).mean())\n",
    "print('CV F1: ', cross_val_score(model, X_train, y_train, scoring=f1_scorer).mean())\n",
    "print('CV Accuracy: ', cross_val_score(model, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
